= Streaming Processing




=== Spark Streaming

Stream processing module for Spark

Processes arriving records in micro-batches
Typically 1 to 10 seconds, though you sould not use time frames that small

image::png/stream_barriers.svg[Straming Batch]

Each micro-batch is an RDD that you can process as if it was a full batch.
These days you can use structured steaming ( Spark 2.4+ ) and use the Data Frame interface.

If you want to build steaming pipelines without code, and without cost use envelope.
Careful not supported but works great, built and suppoted by a few PSAs.

http://github.com/cloudera-labs/envelope

=== Flink

Flink is a framework and distributed processing engine for stateful computations
over unbounded and bounded data streams.

* Unbounded streams have a start but no defined end.
** They do not terminate and provide data as it is generated.
** Unbounded streams must be continuously processed, i.e., events must be promptly handled after they have been ingested.
** It is not possible to wait for all input data to arrive because the input is unbounded and will not be complete
** Processing unbounded data often requires that events are ingested in a specific order, such as the order in which events occurred
* Bounded streams have a defined start and end.
** Bounded streams can be processed by ingesting all data before performing any computations.
** Ordered ingestion is not required to process bounded streams because a bounded data set can always be sorted.
** Processing of bounded streams is also known as batch processing.

At the end of the day it's a API in java and python.

* link:https://ci.apache.org/projects/flink/flink-docs-release-1.10/getting-started/walkthroughs/datastream_api.html[dataset API]
* link:https://ci.apache.org/projects/flink/flink-docs-release-1.10/getting-started/walkthroughs/table_api.html[Table API]



