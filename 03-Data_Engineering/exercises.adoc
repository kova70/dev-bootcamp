
== Data Engineering Exercises

=== Prepare Exercise

Create an Issue called `Data Engineering labs`

* Add it to the Lab milestone
* Label the issue as started
* Assign yourself to the Issue

=== Exercise 1

Deploy a Data Engineering Environment.

* click `Data Hub Clusters`
* click `Create Data Hub`
* Select your Environment
* under `Cluster Template` select `Data Engineering`
* give your cluster a name like : `${github_id}-de`
* add the appropriate tags:

=== Exercise 2

Develop Spark code in your new datahub cluster that performs the following transformations.

1. Joins all the tables
3. Correct data types
4. Adds a flag attribute for gravitational waves

To access hive from Spark you will need to use the link:https://community.cloudera.com/t5/Community-Articles/Integrating-Apache-Hive-with-Apache-Spark-Hive-Warehouse/ta-p/249035[Warehouse Connector] follow the link for some guidance

You can develop this code in either Scala or Python.

TIP: When developing Sacala code use the REPL or Zeplin to prototype your code. Don't try to compile
it every time. You will loose you mind. Once the code works in the REPL create and executable
and run with spark submit.

There is some sample python and scala code in the resources section. I recommend using those for sample code

For SAs extra challenge do it in both languages.

Place the work source code in the file labs/spark.py or labs/spark.scala depending on which
language you chose

=== Exercise 3

Let's start by automating the workflow

Write and execute an Oozie workflow that:

1. runs a hive query to pre-aggregate the tables tables
2. than runs the spark code to transform the data and adds the gravitational waves flag

As a single workflow

Place the oozie workflow file and configuration file in labs/workflow.xml and labs/wf.conf
respectively

=== Exercise 4

Destroy the deployment of Data Engineering datahub before you finish for the day

If you don't do this you fail the course!!!

=== Exercise 5

* Label the issue as in review