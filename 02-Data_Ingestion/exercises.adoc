== Data Ingestion exercises



=== Exercise 1

Create an Issue called Ingestion labs
* Add it to the Lab milestone
* Label the issue as started
* Assign yourself to the Issue

Deploy a data hub cluster with a flow manager template

=== Exercise 2

The data for this project are in the following 5 locations:

Source Type, Table, Source URL

|===
|Source Type| Table| Source URL

|HDFS
|measurements
|https://example.com

|S3
|galaxies
|https://example.com

|RDBMS
|astrophysicists
|https://example.com

|Text File om FTP
|detectors
|https://example.com

|Kafka
|measurements small
|https://example.com
|===

For each of the above sources create a NiFi flow to ingested the data into a hive able

The Data: 500 million measurements, 8 detectors, 128 galaxies, 106 astrophysicists
The Tables: measurements, detectors, galaxies, astrophysicists

=== Exercises 3

Develop the commands to ingest measurements and reference data from Oracle into your cluster using Sqoop.
You can use degree of parallelism 1 for your initial job.

Sqoop should be available as a command line interface at the terminal in any databub instance

Make the tables available in Hive tables for querying

Put the sqoop command used in the file labs/sqoop.adoc